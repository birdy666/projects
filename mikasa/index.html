<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="keywords" content="3D Visual Grounding, Vision-Language Model">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MiKASA: Multi-Key-Anchor & Scene-Aware Transformer for 3D Visual Grounding</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="./static/css/mag.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.jpg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <!-- Loads <model-viewer> for modern browsers: -->
  <script type="module"
  src="https://unpkg.com/@google/model-viewer/dist/model-viewer.js">
  </script>

  <!-- Loads <model-viewer> for old browsers like IE11: -->
  <script nomodule
  src="https://unpkg.com/@google/model-viewer/dist/model-viewer-legacy.js">
  </script>

  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/mag.js"></script>
  <script src="./static/js/video_comparison.js"></script>

  <style>
    .grid-container {
      display: grid;
      grid-template-columns: repeat(6, 1fr);  
      gap: 10px; 
      padding: 0;
    }
  
    .grid-container > div {
      text-align: center;
      padding: 0;
      font-size: 30px;
    }
  
    .item1 {
      grid-column: 1 / 4; 
    }
    .item2 {
      grid-column: 4 / 7;  
    }
    .item3 {
      grid-column: 1 / 4;  
    }
    .item4 {
      grid-column: 4 / 7;  
    }
  </style>
  
</head>
<body>


<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://birdy666.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title" style="margin-bottom: 0"><strong><i>MiKASA:</i></strong></h1>
          <br>
          <h2 class="title is-3 publication-title" style="margin-top: 0; margin-bottom: 0">Multi-Key-Anchor & Scene-Aware Transformer for 3D Visual Grounding</h2>
          <div class="cvpr2024" style="margin-top: 10px; margin-bottom: 20px;">
            <h2 class="title is-4">CVPR 2024</h2>
          </div>

          <!-- <h1 class="title is-1 publication-title">Uni-SLAM: Uncertainty-Aware Neural Implicit SLAM for Real-Time Dense Indoor Scene Reconstruction</h1> -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://birdy666.github.io/">Chun-Peng Chang</a>,
            </span>
            <span class="author-block">
              <a href="https://shaoxiang777.github.io/">Shaoxiang Wang</a>,
            </span>
            <span class="author-block">
              <a href="https://www.dfki.de/en/web/about-us/employee/person/alpa02">Alain Pagani</a>,
            </span>
            <span class="author-block">
              <a href="https://www.dfki.de/en/web/about-us/employee/person/dist01">Didier Stricker</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.dfki.de/en/web/research/research-departments/augmented-vision/">German Research Center for Artificial Intelligence(DFKI)</a>
            </span>   
      
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chang_MiKASA_Multi-Key-Anchor__Scene-Aware_Transformer_for_3D_Visual_Grounding_CVPR_2024_paper.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-widescreen">

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <hr>
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          
          <p>
            3D visual grounding involves matching natural language 
            descriptions with their corresponding objects in 3D spaces. 
            Existing methods often face challenges with accuracy in object 
            recognition and struggle in interpreting complex linguistic queries
             particularly with descriptions that involve multiple anchors 
             or are view-dependent. In response we present the 
             MiKASA (Multi-Key-Anchor Scene-Aware) Transformer. Our novel 
             end-to-end trained model integrates a self-attention-based scene-aware object 
             encoder and an original multi-key-anchor technique enhancing object 
             recognition accuracy and the understanding of spatial relationships. 
             Furthermore MiKASA improves the explainability of decision-making facilitating 
             error diagnosis. Our model achieves the highest overall 
             accuracy in the Referit3D challenge for both the Sr3D and Nr3D datasets 
             particularly excelling by a large margin in categories that require viewpoint-dependent descriptions.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>





<section class="section">
  <div class="container is-max-widescreen">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifth">
        <hr>
        <h2 class="title is-3">Method</h2>
        <br>
        <!-- <embed src="./resources/pipeline.pdf" class="center" width="100%" height="800px"> -->
        <img src="./resources/images/architecture.jpg" class="center" width="100%">
        <div class="content has-text-justified">
          <br>  
          <p>
            Architecture of our 3D Visual Grounding Model, which includes four main modules: 
            a text encoder (Bert), a vision module with a scene-aware object encoder, a spatial
            module that fuses spatial and textual data, and a multi-layered fusion module. The 
            fusion module combines text, spatial, and object features, employing a dual-scoring 
            system for enhanced object category identification and spatial-language assessment.
          </p>
        </div>
      </div>
    <br>
  </div>
</section>

<section class="section">
  <div class="container is-max-widescreen">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifth">
        <hr>
        <h2 class="title is-3">Example</h2>
        <br>
        <!-- <embed src="./resources/pipeline.pdf" class="center" width="100%" height="800px"> -->
        <img src="./resources/images/example.jpg" class="center" width="100%">
        <div class="content has-text-justified">
          <br>  
          <p>
            Visual representation of the model's decision-making process in diverse situations. 
            Rows, from top to bottom, depict: (1) Choices determined by category score, (2) Choices determined by spatial score, 
            (3) Our model's final selection after combining both scores, and 
            (4) The established ground truth. Columns from left to right showcase varying scenarios. 
            The green bounding box refers to the chosen object, and the red bounding box refers to the unchosen distractors.
          </p>
        </div>
      </div>
    <br>
  </div>
</section>

<section class="section">
  <div class="container is-max-widescreen">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifth">
        <hr>
        <h2 class="title is-3">Result</h2>
        <br>
        <!-- <embed src="./resources/pipeline.pdf" class="center" width="100%" height="800px"> -->
        <img src="./resources/images/result.jpg" class="center" width="100%">
        <div class="content has-text-justified">
          <br>  
          <p>
            Our result compared with existing works.
          </p>
        </div>
      </div>
    <br>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{chang2024mikasa,
      title={MiKASA: Multi-Key-Anchor \& Scene-Aware Transformer for 3D Visual Grounding},
      author={Chang, Chun-Peng and Wang, Shaoxiang and Pagani, Alain and Stricker, Didier},
      booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
      pages={14131--14140},
      year={2024}
    }</code></pre>
  </div>
</section>


<section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgements</h2>
    This research has been partially funded by EU project FLUENTLY (GA: Nr 101058680) and the BMBF project SocialWear (01IW20002). The project is built based on the following repository: <a href="https://github.com/referit3d/referit3d">ReferIt3D</a>, 
    <a href="https://github.com/sega-hsj/MVT-3DVG">MVT</a> for their excellent work!
  </div>
</section>



<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            Our project page is built upon <a href="https://github.com/nerfies/nerfies.github.io">source code</a>.
            We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
