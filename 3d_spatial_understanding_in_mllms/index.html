<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="keywords" content="3D Spatial Understanding in MLLMs: Disambiguation and Evaluation>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>3D Spatial Understanding in MLLMs: Disambiguation and Evaluation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="./static/css/mag.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.jpg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <!-- Loads <model-viewer> for modern browsers: -->
  <script type="module"
  src="https://unpkg.com/@google/model-viewer/dist/model-viewer.js">
  </script>

  <!-- Loads <model-viewer> for old browsers like IE11: -->
  <script nomodule
  src="https://unpkg.com/@google/model-viewer/dist/model-viewer-legacy.js">
  </script>

  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/mag.js"></script>
  <script src="./static/js/video_comparison.js"></script>

  <style>
    .grid-container {
      display: grid;
      grid-template-columns: repeat(6, 1fr);  
      gap: 10px; 
      padding: 0;
    }
  
    .grid-container > div {
      text-align: center;
      padding: 0;
      font-size: 30px;
    }
  
    .item1 {
      grid-column: 1 / 4; 
    }
    .item2 {
      grid-column: 4 / 7;  
    }
    .item3 {
      grid-column: 1 / 4;  
    }
    .item4 {
      grid-column: 4 / 7;  
    }
  </style>
  
</head>
<body>


<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://chunpeng-chang.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h2 class="title is-3 publication-title" style="margin-top: 0; margin-bottom: 0">3D Spatial Understanding in MLLMs: Disambiguation and Evaluation</h2>
          <div class="icra2025" style="margin-top: 10px; margin-bottom: 20px;">
            <h2 class="title is-4">ICRA 2025</h2>
          </div>

          <!-- <h1 class="title is-1 publication-title">Uni-SLAM: Uncertainty-Aware Neural Implicit SLAM for Real-Time Dense Indoor Scene Reconstruction</h1> -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://chunpeng-chang.github.io/">Chun-Peng Chang</a>,
            </span>
            <span class="author-block">
              <a href="https://www.dfki.de/en/web/about-us/employee/person/alpa02">Alain Pagani</a>,
            </span>
            <span class="author-block">
              <a href="https://www.dfki.de/en/web/about-us/employee/person/dist01">Didier Stricker</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.dfki.de/en/web/research/research-departments/augmented-vision/">German Research Center for Artificial Intelligence(DFKI)</a>
            </span>   
      
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2412.06613"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
                <a href="https://github.com/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code(soon)</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-widescreen">

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <iframe width="800" height="430"
        src="https://www.youtube.com/embed/AIdsC_oKzLY">
        </iframe>
        <hr>
        <img src="./static/images/teaser.png" class="center" width="100%">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Multimodal Large Language Models (MLLMs) have made significant progress in tasks 
            such as image captioning and question answering. However, while these models can 
            generate realistic captions, they often struggle with providing precise instructions, 
            particularly when it comes to localizing and disambiguating objects in complex 3D 
            environments. This capability is critical as MLLMs become more integrated with 
            collaborative robotic systems. In scenarios where a target object is surrounded by 
            similar objects (distractors), robots must deliver clear, spatially-aware instructions 
            to guide humans effectively. We refer to this challenge as contextual object localization 
            and disambiguation, which imposes stricter constraints than conventional 3D dense 
            captioning, especially regarding ensuring target exclusivity.
            In response, we propose simple yet effective techniques to enhance the model's ability to 
            localize and disambiguate target objects. Our approach not only achieves state-of-the-art 
            performance on conventional metrics that evaluate sentence similarity, but also demonstrates 
            improved 3D spatial understanding through 3D visual grounding model. The code will be released 
            upon acceptance of our work.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-widescreen">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifth">
        <hr>
        <h2 class="title is-3">Contextual Object Localization and Disambiguation
          (COLD) </h2>
        <br>
        The COLD task requires a robot equipped with a Multimodal Large Language Model (MLLM) to guide a human collaborator in localizing a specific target object among multiple similar ones (distractors) in complex 3D environments. Unlike traditional captioning, this task not only demands generating a description for an object, but also ensuring the instruction is exclusive to the target and effectively disambiguates it from others. This capability is critical for real-world human-robot collaboration, where precise and unambiguous guidance is essential for effective interaction.
      </div>
    <br>
  </div>
</section>

<section class="section">
  <div class="container is-max-widescreen">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifth">
        <hr>
        <h2 class="title is-3">Method</h2>
        <br>
        <!-- <embed src="./resources/pipeline.pdf" class="center" width="100%" height="800px"> -->
        <img src="./static/images/architecture.png" class="center" width="100%">
        <div class="content has-text-justified">
          <p>
            Our training and evaluation pipeline consists of four key steps: visual input encoding, VLM token encoding, LLM generation, and evaluation. The input to the system includes a point cloud and a target ID, which typically comes from upstream tasks such as robot assistants or human-robot teaching interactions, could be a bounding box or just an ID number. In the visual input encoding step, we identify distractors based on the point cloud features of the target object and encode the relative spatial relationships between the target, distractors, and potential anchors. Various token encoding techniques are then applied. During evaluation, we assess the quality of the generated spatial instructions by measuring both sentence similarity and deeper understanding of 3D spatial comprehension.
          </p>
        </div>
      </div>
    <br>
  </div>
</section>

<section class="section">
  <div class="container is-max-widescreen">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifth">
        <hr>
        <h2 class="title is-3">Insufficiency of Existing Metrics on Evaluating 3D Understanding</h2>
        <br>
        <!-- <embed src="./resources/pipeline.pdf" class="center" width="100%" height="800px"> -->
        <img src="./static/images/metric_limitation.png" class="center" width="100%">
        <div class="content has-text-justified">
          <p>
            Previous works evaluate their performance using metrics such as BLEU, ROUGE, METEOR, CIDEr, and SPICE. While these metrics have significantly contributed to progress in natural language processing—by focusing on surface-level text matching through n-gram overlap, semantic similarity, and lexical fidelity—they often fall short when applied to MLLMs that handle multimodal data and complex spatial reasoning.
          </p>
          <p>
            The figure illustrates the limitations of conventional metrics. For instance, these metrics assign a high score to text <i>IV</i>, despite its reference to a hallucinated "blue table." Similarly, text <i>V</i> receives a high score even though its critical spatial descriptor "far" directly contradicts the "next" in the reference. Text <i>III</i>, although accurate, is ambiguous because it fails to resolve the presence of multiple nightstands next to a bed, making it ineffective for precise localization. On the other hand, texts <i>I</i> and <i>II</i>, which provide more contextually appropriate anchor references, receive lower scores—highlighting how these metrics overlook spatial accuracy.
          </p>
          <p>
            Even advanced metrics like METEOR, CIDEr, and SPICE, which attempt to capture semantic meaning and sentence uniqueness, encounter similar shortcomings in the context of our task.
          </p>
        </div>
      </div>
    <br>
  </div>
</section>

<section class="section">
  <div class="container is-max-widescreen">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifth">
        <hr>
        <h2 class="title is-3">Evaluation on Existing Metrics</h2>
        <br>
        <!-- <embed src="./resources/pipeline.pdf" class="center" width="100%" height="800px"> -->
        <img src="./static/images/eval_convention.png" class="center" width="70%">
        <div class="content has-text-justified">
          <p>
            Performance comparison of different models on the Nr3D and Sr3D test datasets, showing that our approach outperforms Vote2Cap++ across several metrics.
However, we also observe that while these conventional metrics effectively measure sentence similarity, they fail to capture the nuanced 3D spatial understanding. In the Sr3D evaluation, we manually replaced all words related to spatial relationships with either <i>"far"</i> or <i>"close"</ii>, resulting in only a slight drop in performance. This suggests that n-gram-based evaluations alone may not be sufficient to fully assess a model's ability to generate precise instructions that can disambiguate target objects from distractors.
          </p>
        </div>
      </div>
    <br>
  </div>
</section>

<section class="section">
  <div class="container is-max-widescreen">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifth">
        <hr>
        <h2 class="title is-3">Evaluation with 3D Visual Grounding</h2>
        <br>
        <!-- <embed src="./resources/pipeline.pdf" class="center" width="100%" height="800px"> -->
        <img src="./static/images/eval_3dvg.png" class="center" width="50%">
        <div class="content has-text-justified">
          <p>
            To address the limitations of existing metrics, we use the text generated by the MLLM as synthetic training data to train various 3D Visual Grounding (3DVG) models. The underlying idea is that if the generated instructions are human-like and accurate enough, then 3DVG models should be able to learn from them and perform well on human-annotated test sets.
          </p>
          
          <p>
            Comparisons with human-created ground truth data show that our model generates textual instructions that not only closely resemble human-written descriptions but also enable 3DVG models to develop spatial understanding in a human-like manner.
          </p>
          
          <p>
            While we acknowledge that this evaluation may be influenced by the current limitations of 3DVG models, we believe that combining conventional metrics with 3DVG-based evaluation offers a more comprehensive and robust analysis. As 3DVG models continue to advance, we anticipate that this evaluation strategy will become even more reliable and effective in the near future.
          </p>
        </div>
      </div>
    <br>
  </div>
</section>

<section class="section">
  <div class="container is-max-widescreen">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifth">
        <hr>
        <h2 class="title is-3">Qualitative Results</h2>
        <br>
        <!-- <embed src="./resources/pipeline.pdf" class="center" width="100%" height="800px"> -->
        <img src="./static/images/qualitative_results.png" class="center" width="100%">
        <div class="content has-text-justified">
          <p>

          </p>
        </div>
      </div>
    <br>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{chang20243d,
      title={3D Spatial Understanding in MLLMs: Disambiguation and Evaluation},
      author={Chang, Chun-Peng and Pagani, Alain and Stricker, Didier},
      journal={arXiv preprint arXiv:2412.06613},
      year={2024}
    }</code></pre>
  </div>
</section>

<section class="section" id="Acknowledgements"></section>
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgements</h2>
    This research has been partially funded by EU project FLUENTLY (GA: Nr 101058680) and ExtremeXP (GA: Nr 101093164).
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            Our project page is built upon <a href="https://github.com/nerfies/nerfies.github.io">source code</a>.
            We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
